{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "from onpolicy.algorithms.r_mappo.algorithm.intention_sharing import IntentionSharingModel\n",
    "from onpolicy.envs.env_wrappers import SubprocVecEnv, DummyVecEnv\n",
    "from onpolicy.config import get_config\n",
    "\n",
    "from pettingzoo.sisl import pursuit_v4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_args(args_dict):  \n",
    "    args = argparse.Namespace()\n",
    "    for key, value in args_dict.items():\n",
    "        setattr(args, key, value)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "        'env_name': \"SISL\",\n",
    "        'use_obs_instead_of_state': False,\n",
    "        'scenario_name': 'pursuit',\n",
    "        'num_agents': 3,\n",
    "        'num_landmarks': 3, \n",
    "        'n_training_threads': 1,\n",
    "        'n_rollout_threads': 1,\n",
    "        'episode_length': 25,\n",
    "        'use_local_obs': False,\n",
    "        'seed': 1,\n",
    "        'hidden_size': 64,\n",
    "        'recurrent_N': 2,\n",
    "        'gain': 0.01,\n",
    "        'use_orthogonal': True,\n",
    "        'use_policy_active_masks': False,\n",
    "        'use_naive_recurrent_policy': False,\n",
    "        'use_recurrent_policy': False,\n",
    "        'intention_aggregation': 'mean',\n",
    "        'imagined_traj_len': 4,\n",
    "        'communication_interval': 4,\n",
    "        'use_feature_normalization': True,\n",
    "        'use_ReLU': True,\n",
    "        'stacked_frames': 1,\n",
    "        'layer_N': 1,\n",
    "        \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PettingZooToOnPolicyWrapper(gym.Env):\n",
    "\n",
    "    def __init__(self, env, seed):\n",
    "        self.env = env\n",
    "        self.seed = seed\n",
    "        self.n = self.env.max_num_agents\n",
    "        obs_shape = self.env.observation_space(self.env.possible_agents[0]).shape\n",
    "        self.observation_space = [self.env.observation_space(agent) for agent in self.env.possible_agents]\n",
    "        self.action_space = [self.env.action_space(agent) for agent in self.env.possible_agents]\n",
    "        self.share_observation_space = spaces.Box(-np.inf, np.inf, (self.n, *obs_shape))\n",
    "\n",
    "    def reset(self):\n",
    "        obs, _ = self.env.reset(self.seed)\n",
    "        obs = self._dict_to_array(obs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, actions):\n",
    "        actions = self._convert_actions(actions)\n",
    "        actions = self._array_to_dict(actions)\n",
    "        obs, rewards, terminated, truncated, info = self.env.step(actions)\n",
    "\n",
    "        obs = self._dict_to_array(obs)\n",
    "        rewards = self._dict_to_array(rewards)\n",
    "        rewards = [[r] for r in rewards]\n",
    "        terminated = self._dict_to_array(terminated)\n",
    "        truncated = self._dict_to_array(truncated)\n",
    "        terminated = [a or b for a, b in zip(terminated, truncated)]\n",
    "        info = self._dict_to_array(info)\n",
    "        return obs, rewards, terminated, info\n",
    "    \n",
    "    def _convert_actions(self, actions):\n",
    "        converted_actions = []\n",
    "        for a in actions:\n",
    "            converted_actions.append(np.argmax(a))\n",
    "        return converted_actions\n",
    "\n",
    "    def _dict_to_array(self, d):\n",
    "        a = []\n",
    "        for agent in self.env.possible_agents:\n",
    "            a.append(d[agent])\n",
    "        return a\n",
    "    \n",
    "    def _array_to_dict(self, a):\n",
    "        d = {}\n",
    "        for agent, value in zip(self.env.possible_agents, a):\n",
    "            d[agent] = value\n",
    "        return d\n",
    "\n",
    "    \n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.env, attr)\n",
    "    \n",
    "def SISLEnv(args, seed):\n",
    "    if args.scenario_name == 'pursuit':\n",
    "        return PettingZooToOnPolicyWrapper(pursuit_v4.parallel_env(max_cycles=args.episode_length, n_pursuers=args.num_agents), seed)\n",
    "    else:\n",
    "        NotImplementedError(f'{args.scenario_name} is not implemented in SISL.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_env(all_args):\n",
    "    def get_env_fn(rank):\n",
    "        def init_env():\n",
    "            if all_args.env_name == \"SISL\":\n",
    "                env = SISLEnv(all_args, seed=all_args.seed + rank * 1000)\n",
    "            else:\n",
    "                print(\"Can not support the \" + all_args.env_name + \"environment.\")\n",
    "                raise NotImplementedError\n",
    "            return env\n",
    "\n",
    "        return init_env\n",
    "\n",
    "    if all_args.n_rollout_threads == 1:\n",
    "        return DummyVecEnv([get_env_fn(0)])\n",
    "    else:\n",
    "        return SubprocVecEnv([get_env_fn(i) for i in range(all_args.n_rollout_threads)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = make_args(args_dict)\n",
    "envs = make_train_env(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "envs.reset()\n",
    "one_hot_actions = [np.array([0,0,0,0,1]) for _ in range(3)]\n",
    "step = 0\n",
    "while True:\n",
    "    step += 1\n",
    "    if step == 25:\n",
    "        print(\"stop\")\n",
    "    obs, rews, dones, infos = envs.step(one_hot_actions)\n",
    "    if np.any(dones):\n",
    "        print('done')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_samples(action_spaces):\n",
    "    sample = []\n",
    "    for action_space in action_spaces:\n",
    "        sample.append(action_space.sample())\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penv = SISLEnv(args, 4)\n",
    "penv.reset()\n",
    "one_hot_actions = [np.array([0,0,0,0,1]) for _ in range(penv.max_num_agents)]\n",
    "penv.step(one_hot_actions)\n",
    "# penv.step(get_action_samples(penv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
