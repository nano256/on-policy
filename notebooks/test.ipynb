{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8519)\n",
      "tensor(1.8519)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5)).reshape((1,-1))\n",
    "y1 = torch.Tensor([0,0,0,1,0]).reshape((1,-1))\n",
    "y2 = torch.Tensor([3]).long()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(loss_fn(x, y2))\n",
    "print(loss_fn(x, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    assert len(p.shape) == 1\n",
    "    assert len(q.shape) == 1\n",
    "    assert len(p) == len(q)\n",
    "\n",
    "    cross_entropy = 0\n",
    "    for idx in range(len(p)):\n",
    "        cross_entropy += (p[idx] * np.log(q[idx])).item()\n",
    "    return -cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09219617436462002\n",
      "4.650858815019799\n",
      "0.5602362463969972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michel/miniconda3/envs/test-ppo/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/michel/miniconda3/envs/test-ppo/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"\n",
      "/home/michel/miniconda3/envs/test-ppo/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n",
      "/home/michel/miniconda3/envs/test-ppo/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == \"__main__\":\n"
     ]
    }
   ],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "p = np.array([100, 1, 1], dtype=np.float)\n",
    "p = p / np.linalg.norm(p)\n",
    "q1 = np.array([100, 1, 1], dtype=np.float)\n",
    "q1 = q1 / np.linalg.norm(q1)\n",
    "q2 = np.array([1, 1, 100], dtype=np.float)\n",
    "q2 = q2 / np.linalg.norm(q2)\n",
    "q3 = np.array([1, 1, 1], dtype=np.float)\n",
    "q3 = q3 / np.linalg.norm(q3)\n",
    "\n",
    "print(cross_entropy(p, q1))\n",
    "print(cross_entropy(p, q2))\n",
    "print(cross_entropy(p, q3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=128, num_hidden=2):\n",
    "        nn.Module.__init__(self)\n",
    "        activation_fn = nn.ReLU\n",
    "        layers = [\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            activation_fn(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "        ]\n",
    "        for _ in range(num_hidden):\n",
    "            layers.extend(\n",
    "                [\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    activation_fn(),\n",
    "                    nn.LayerNorm(hidden_size),\n",
    "                ]\n",
    "            )\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "\n",
    "class EncoderDecoderModule(nn.Module):\n",
    "    def __init__(self, input_size, latent_space_size, args):\n",
    "        super().__init__()\n",
    "        self.encoder = MLP(\n",
    "            input_size, latent_space_size, args.hidden_size, args.layer_N\n",
    "        )\n",
    "        self.decoder = MLP(\n",
    "            latent_space_size, input_size, args.hidden_size, args.layer_N\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc_forward(x)\n",
    "        return self.dec_forward(z)\n",
    "\n",
    "    def enc_forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def dec_forward(self, z):\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_enc_dec(\n",
    "        policy, obs_size, act_size, traj_len, n_samples, batch_size, n_episodes\n",
    "):\n",
    "    n_batches = n_samples // batch_size\n",
    "    model = policy\n",
    "    optim = torch.optim.Adam(model.parameters())\n",
    "    obs_loss_fn = nn.MSELoss()\n",
    "    act_loss_fn = nn.CrossEntropyLoss()\n",
    "    steps = 0\n",
    "\n",
    "    step_size = obs_size + act_size\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        for _ in range(n_batches):\n",
    "            optim.zero_grad()\n",
    "            x = torch.rand((batch_size, step_size * traj_len))\n",
    "            y_obs = x.reshape(-1, step_size)[:, :obs_size]\n",
    "            y_act = F.softmax(x.reshape(-1, step_size)[:, -act_size:], 1)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            y_pred_obs = y_pred.reshape(-1, step_size)[:, :obs_size]\n",
    "            y_pred_act = y_pred.reshape(-1, step_size)[:, -act_size:]\n",
    "\n",
    "            obs_loss = obs_loss_fn(y_pred_obs, y_obs)\n",
    "            act_loss = act_loss_fn(y_pred_act, y_act)\n",
    "            loss = obs_loss + act_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            print(f\"Obs loss at step {steps}: {obs_loss:.4f}\")\n",
    "            print(f\"Act loss at step {steps}: {act_loss:.4f}\")\n",
    "            # self.log_train({\"enc_dec_pretrain_loss\": loss.item()}, steps)\n",
    "            # steps += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "obs_size = 18\n",
    "act_size = 5\n",
    "traj_len = 5\n",
    "input_size = (obs_size + act_size) * traj_len\n",
    "latent_space_size = 64\n",
    "args = Object()\n",
    "args.hidden_size = 128\n",
    "args.layer_N = 4\n",
    "model = EncoderDecoderModule(input_size, latent_space_size, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs loss at step 0: 0.0827\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.1247\n",
      "Act loss at step 0: 1.6266\n",
      "Obs loss at step 0: 0.0938\n",
      "Act loss at step 0: 1.6136\n",
      "Obs loss at step 0: 0.0933\n",
      "Act loss at step 0: 1.6138\n",
      "Obs loss at step 0: 0.0947\n",
      "Act loss at step 0: 1.6138\n",
      "Obs loss at step 0: 0.0940\n",
      "Act loss at step 0: 1.6121\n",
      "Obs loss at step 0: 0.0926\n",
      "Act loss at step 0: 1.6116\n",
      "Obs loss at step 0: 0.0871\n",
      "Act loss at step 0: 1.6122\n",
      "Obs loss at step 0: 0.0886\n",
      "Act loss at step 0: 1.6121\n",
      "Obs loss at step 0: 0.0882\n",
      "Act loss at step 0: 1.6124\n",
      "Obs loss at step 0: 0.0889\n",
      "Act loss at step 0: 1.6117\n",
      "Obs loss at step 0: 0.0884\n",
      "Act loss at step 0: 1.6104\n",
      "Obs loss at step 0: 0.0872\n",
      "Act loss at step 0: 1.6103\n",
      "Obs loss at step 0: 0.0880\n",
      "Act loss at step 0: 1.6101\n",
      "Obs loss at step 0: 0.0864\n",
      "Act loss at step 0: 1.6104\n",
      "Obs loss at step 0: 0.0875\n",
      "Act loss at step 0: 1.6102\n",
      "Obs loss at step 0: 0.0843\n",
      "Act loss at step 0: 1.6109\n",
      "Obs loss at step 0: 0.0865\n",
      "Act loss at step 0: 1.6105\n",
      "Obs loss at step 0: 0.0862\n",
      "Act loss at step 0: 1.6105\n",
      "Obs loss at step 0: 0.0865\n",
      "Act loss at step 0: 1.6103\n",
      "Obs loss at step 0: 0.0852\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0857\n",
      "Act loss at step 0: 1.6099\n",
      "Obs loss at step 0: 0.0836\n",
      "Act loss at step 0: 1.6101\n",
      "Obs loss at step 0: 0.0861\n",
      "Act loss at step 0: 1.6100\n",
      "Obs loss at step 0: 0.0849\n",
      "Act loss at step 0: 1.6100\n",
      "Obs loss at step 0: 0.0848\n",
      "Act loss at step 0: 1.6100\n",
      "Obs loss at step 0: 0.0845\n",
      "Act loss at step 0: 1.6102\n",
      "Obs loss at step 0: 0.0860\n",
      "Act loss at step 0: 1.6094\n",
      "Obs loss at step 0: 0.0855\n",
      "Act loss at step 0: 1.6096\n",
      "Obs loss at step 0: 0.0838\n",
      "Act loss at step 0: 1.6096\n",
      "Obs loss at step 0: 0.0843\n",
      "Act loss at step 0: 1.6100\n",
      "Obs loss at step 0: 0.0836\n",
      "Act loss at step 0: 1.6096\n",
      "Obs loss at step 0: 0.0859\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0857\n",
      "Act loss at step 0: 1.6095\n",
      "Obs loss at step 0: 0.0841\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0849\n",
      "Act loss at step 0: 1.6099\n",
      "Obs loss at step 0: 0.0847\n",
      "Act loss at step 0: 1.6100\n",
      "Obs loss at step 0: 0.0840\n",
      "Act loss at step 0: 1.6100\n",
      "Obs loss at step 0: 0.0838\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0848\n",
      "Act loss at step 0: 1.6096\n",
      "Obs loss at step 0: 0.0837\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0851\n",
      "Act loss at step 0: 1.6098\n",
      "Obs loss at step 0: 0.0837\n",
      "Act loss at step 0: 1.6098\n",
      "Obs loss at step 0: 0.0840\n",
      "Act loss at step 0: 1.6095\n",
      "Obs loss at step 0: 0.0827\n",
      "Act loss at step 0: 1.6095\n",
      "Obs loss at step 0: 0.0834\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0853\n",
      "Act loss at step 0: 1.6093\n",
      "Obs loss at step 0: 0.0840\n",
      "Act loss at step 0: 1.6094\n",
      "Obs loss at step 0: 0.0828\n",
      "Act loss at step 0: 1.6097\n",
      "Obs loss at step 0: 0.0847\n",
      "Act loss at step 0: 1.6093\n",
      "Obs loss at step 0: 0.0842\n",
      "Act loss at step 0: 1.6094\n",
      "Obs loss at step 0: 0.0824\n",
      "Act loss at step 0: 1.6089\n",
      "Obs loss at step 0: 0.0832\n",
      "Act loss at step 0: 1.6092\n",
      "Obs loss at step 0: 0.0832\n",
      "Act loss at step 0: 1.6094\n",
      "Obs loss at step 0: 0.0832\n",
      "Act loss at step 0: 1.6088\n",
      "Obs loss at step 0: 0.0831\n",
      "Act loss at step 0: 1.6090\n",
      "Obs loss at step 0: 0.0829\n",
      "Act loss at step 0: 1.6095\n",
      "Obs loss at step 0: 0.0818\n",
      "Act loss at step 0: 1.6093\n",
      "Obs loss at step 0: 0.0829\n",
      "Act loss at step 0: 1.6094\n",
      "Obs loss at step 0: 0.0835\n",
      "Act loss at step 0: 1.6090\n",
      "Obs loss at step 0: 0.0826\n",
      "Act loss at step 0: 1.6092\n",
      "Obs loss at step 0: 0.0837\n",
      "Act loss at step 0: 1.6090\n",
      "Obs loss at step 0: 0.0832\n",
      "Act loss at step 0: 1.6096\n",
      "Obs loss at step 0: 0.0830\n",
      "Act loss at step 0: 1.6092\n",
      "Obs loss at step 0: 0.0836\n",
      "Act loss at step 0: 1.6085\n",
      "Obs loss at step 0: 0.0847\n",
      "Act loss at step 0: 1.6090\n",
      "Obs loss at step 0: 0.0836\n",
      "Act loss at step 0: 1.6098\n",
      "Obs loss at step 0: 0.0835\n",
      "Act loss at step 0: 1.6087\n",
      "Obs loss at step 0: 0.0830\n",
      "Act loss at step 0: 1.6091\n",
      "Obs loss at step 0: 0.0812\n",
      "Act loss at step 0: 1.6087\n",
      "Obs loss at step 0: 0.0839\n",
      "Act loss at step 0: 1.6088\n",
      "Obs loss at step 0: 0.0835\n",
      "Act loss at step 0: 1.6089\n",
      "Obs loss at step 0: 0.0818\n",
      "Act loss at step 0: 1.6085\n",
      "Obs loss at step 0: 0.0833\n",
      "Act loss at step 0: 1.6090\n",
      "Obs loss at step 0: 0.0821\n",
      "Act loss at step 0: 1.6094\n",
      "Obs loss at step 0: 0.0842\n",
      "Act loss at step 0: 1.6086\n",
      "Obs loss at step 0: 0.0845\n",
      "Act loss at step 0: 1.6084\n",
      "Obs loss at step 0: 0.0826\n",
      "Act loss at step 0: 1.6084\n",
      "Obs loss at step 0: 0.0846\n",
      "Act loss at step 0: 1.6088\n",
      "Obs loss at step 0: 0.0840\n",
      "Act loss at step 0: 1.6084\n",
      "Obs loss at step 0: 0.0842\n",
      "Act loss at step 0: 1.6079\n",
      "Obs loss at step 0: 0.0832\n",
      "Act loss at step 0: 1.6081\n",
      "Obs loss at step 0: 0.0843\n",
      "Act loss at step 0: 1.6079\n",
      "Obs loss at step 0: 0.0820\n",
      "Act loss at step 0: 1.6076\n",
      "Obs loss at step 0: 0.0830\n",
      "Act loss at step 0: 1.6079\n",
      "Obs loss at step 0: 0.0832\n",
      "Act loss at step 0: 1.6081\n",
      "Obs loss at step 0: 0.0825\n",
      "Act loss at step 0: 1.6081\n",
      "Obs loss at step 0: 0.0818\n",
      "Act loss at step 0: 1.6075\n",
      "Obs loss at step 0: 0.0831\n",
      "Act loss at step 0: 1.6070\n",
      "Obs loss at step 0: 0.0830\n",
      "Act loss at step 0: 1.6080\n",
      "Obs loss at step 0: 0.0816\n",
      "Act loss at step 0: 1.6075\n",
      "Obs loss at step 0: 0.0806\n",
      "Act loss at step 0: 1.6078\n",
      "Obs loss at step 0: 0.0803\n",
      "Act loss at step 0: 1.6075\n",
      "Obs loss at step 0: 0.0815\n",
      "Act loss at step 0: 1.6084\n",
      "Obs loss at step 0: 0.0824\n",
      "Act loss at step 0: 1.6085\n",
      "Obs loss at step 0: 0.0822\n",
      "Act loss at step 0: 1.6076\n",
      "Obs loss at step 0: 0.0823\n",
      "Act loss at step 0: 1.6079\n",
      "Obs loss at step 0: 0.0813\n",
      "Act loss at step 0: 1.6076\n",
      "Obs loss at step 0: 0.0817\n",
      "Act loss at step 0: 1.6076\n",
      "Obs loss at step 0: 0.0825\n",
      "Act loss at step 0: 1.6074\n"
     ]
    }
   ],
   "source": [
    "pretrain_enc_dec(model, obs_size, act_size, traj_len, 1000, 100, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
